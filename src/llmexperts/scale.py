import os
import numpy as np
import pandas as pd

from datetime import datetime
from langchain_core.load import dumps

from . import openai_model_list
from .model import LLMClient
from .prompts import ScalePromptTemplate, ScalePrompt


def validate_score(score):
    return score in ['NA', '1', '2', '3', '4', '5', '6', '7']

def scale_text_with_batch(
        prompt_list: list[ScalePrompt], model, parse_retries=3, max_retries=7, concurrency=3,
        probabilities=False, dry_run=False,
        res_persona="index", res_encouragement="index"
):
    """
    Scales the given text, given a list of prompts using the specified model.

    Args:
        prompt_list (list): A list of lists, each containing message objects representing the conversation.
        model (str): The name or ID of the model to use for scale.
        parse_retries (int): The number of times to retry parsing the response. Defaults to 3.
        max_retries (int): The number of times to retry invoking the model. Defaults to 7, which should be enough
            to handle most TPM rate limits with langchains built in exponential backoff.
        concurrency (int): The number of concurrent requests to make to the model. Defaults to 3.
        probabilities (bool): Whether to include token probabilities in the response. Defaults to False. Only works with OpenAI models.
        dry_run (bool, optional): Don't invoke the LLM api call. Return a mock response for debug and testing. Defaults to False.
        res_persona (str, optional): "text": include persona text in result. "index": include persona index in result.
        res_encouragement (str, optional): "text": include encouragement text in result. "index": include encouragement index in result.

    Returns:
        dict: A dictionary containing the scaled text generated by the model.

    """

    llm = LLMClient(model, max_tokens=150, temperature=0, max_retries=max_retries)

    # This is the core call to the model, using batch for concurrency
    # We needed to add concurrency because we hit rate limits with the API
    responses = []
    prompt_batches = [[p.prompt for p in prompt_list[i:i + concurrency]]
                      for i in range(0, len(prompt_list), concurrency)]

    for prompt_batch in prompt_batches:
        responses += llm.batch(prompt_batch, dry_run=dry_run, dry_run_res="NA")

    # This is hardcoded to expect a single score or NA in the response
    # If the desired response changes this will need to be updated
    # Originally this handled a json response but that was removed to make
    # this more robust.
    response_dicts = []
    responses_logs = []
    for i, response in enumerate(responses):
        response_parse = [response]
        try:
            score = response.content.strip()
            if not validate_score(score):
                raise ValueError(f'Invalid score: {score}')
            response_dict = {
                'score': score,
                'error_message': None,
                'prompt': dumps(prompt_list[i])
            }
            if res_persona == "text":
                response_dict["persona"] = prompt_list[i].persona
            else:
                response_dict["persona"] = prompt_list[i].persona_idx

            if res_encouragement == "text":
                response_dict["encouragement"] = prompt_list[i].encouragement
            else:
                response_dict["encouragement"] = prompt_list[i].encouragement_idx

            if probabilities and (model in openai_model_list):
                try:
                    score = response_dict['score']
                    response_meta_df = pd.DataFrame(response.response_metadata["logprobs"]["content"])
                    score_metadata = response_meta_df[response_meta_df['token'] == str(score)].iloc[0]
                    # note this is a little fragile, it will retrieve the probability of the first token in the response
                    # that matches the score, which given the template "should" be the score itself, but it's not guaranteed
                    prob = np.exp(score_metadata['logprob'])
                    response_dict['prob'] = prob
                except Exception as e:
                    print(f'Error extracting probabilities from model {model}: {e}')
                    response_dict['prob'] = 'ERR'
        except Exception as original_error:
            attempt = 1
            while attempt <= parse_retries:
                print(
                    f'\nError parsing response from model {model}, retrying attempt {attempt}')
                try:
                    response = llm.invoke(prompt_list[i])
                    response_parse.append(response)
                    score = response.content.strip()
                    if not validate_score(score):
                        raise ValueError(f'Invalid score: {score}')
                    response_dict = {
                        'score': score, 'error_message': None,
                        'prompt': dumps(prompt_list[i]),
                        'persona': prompt_list[i].persona,
                        'encouragement': prompt_list[i].encouragement,
                    }
                    break
                except:
                    attempt += 1
            else:
                print(f'Retries failed with model {model}: {original_error}')
                response_dict = {
                    'score': 'ERR', 'error_message': original_error,
                    'prompt': dumps(prompt_list[i]),
                    'persona': prompt_list[i].persona,
                    'encouragement': prompt_list[i].encouragement,
                }
        responses_logs.append(response_parse)
        response_dict["responses"] = [dumps(r) for r in responses_logs]
        response_dicts.append(response_dict)
    return response_dicts


def ensure_output_paths(
        output_dir=None,
        results_filepath=None,
        results_filename="scale_results.csv",
):
    if results_filepath is None:
        if not results_filename.endswith('.csv'):
            raise ValueError(f'results_filename must end with .csv')
        if output_dir is None:
            raise ValueError('Either output_dir or result_filepath must be provided')
        if not os.path.exists(output_dir):
            os.makedirs(output_dir)
        results_filepath = os.path.join(output_dir, results_filename)
    else:
        if not results_filepath.endswith('.csv'):
            raise ValueError('results_filepath must end with .csv')
        if not os.path.exists(os.path.dirname(results_filepath)):
            os.makedirs(os.path.dirname(results_filepath))

    return results_filepath


def scale_file(
        filepath, model_list, issue_list, prompt_template: ScalePromptTemplate | os.PathLike, output_dir=None,
        parse_retries=3, max_retries=7, concurrency=3, probabilities=False,
        use_examples=False, override_personas=None, override_encouragements=None, dry_run=False,
        results_filepath=None, results_filename="scale_results.csv",
        save_log=True, meta_columns:dict=None, skip_existing_scale_results=True,
        res_persona="index", res_encouragement="index"
):
    """
    Scales a collection of text files using different models and prompts and output results in a single file.

    Args:
        filepath (str): Path to the file to be scaled.
        model_list (list): A list of model names to use for scale.
        issue_list (list): A list of issue areas corresponding to each text file.
        prompt_template (ScalePromptTemplate|os.PathLike): A SummarizePrompt instance or a filepath to the prompt file.
        output_dir (str): The path to the output directory where the results will be saved.
        parse_retries (int): The number of times to retry parsing the response. Defaults to 3.
        max_retries (int): The number of times to retry invoking the model. Defaults to 7, which should be enough
        to handle most TPM rate limits with langchains built in exponential backoff.
        concurrency (int): The number of concurrent requests to make to the model. Defaults to 3.
        probabilities (bool): Whether to include token probabilities in the response. Defaults to False. Only works with OpenAI models.
        use_examples (bool): Whether to add examples to the prompts. Defaults to False. Examples must be specified in the prompt_template.
        override_personas (int|list[int]): An index or a list of indices of personas to use. Persona texts will be fetched from the prompt template. If None, will use all personas in the template.
        override_encouragements (int|list[int]): An index or a list of indices of encouragements to use. encouragement texts will be fetched from the prompt template. If None, will use all encouragements in the template.
        dry_run (bool): Don't invoke the LLM api call. Return a mock response for debug and testing. Defaults to False.
        results_filepath (str): The path to the csv file where the results will be saved, if provided,
            output_dir and results_file_name will be ignored.
        results_filename (str): The name of the output file. Defaults to 'scale_results.xlsx'.
        save_log (bool): Should the log information be saved to a file. Defaults to False.
        meta_columns (dict): A dictionary of {column_name:value} which will be added to the final result file.
        skip_existing_scale_results (bool): Whether to skip existing scale_results that are already in the specified result file.
        res_persona (str): "text": include persona text in result. "index": include persona index in result.
        res_encouragement (str): "text": include encouragement text in result. "index": include encouragement index in result.

    Returns:
        DataFrame: A DataFrame containing the scaled text generated by each model.

    """
    if not meta_columns:
        meta_columns = {}

    results_filepath = ensure_output_paths(
        output_dir=output_dir,
        results_filepath=results_filepath, results_filename=results_filename
    )

    summary_filename = os.path.basename(filepath)

    overall_results = []
    # Loop through each file, issue area, model and prompt
    if not isinstance(prompt_template, ScalePromptTemplate):
        prompt_template = ScalePromptTemplate.from_file(prompt_template)

    # for file_name in file_list:
    print('--------\nScaling file: ', summary_filename)
    with open(filepath, "r", encoding="utf-8") as file:
        text = file.read()

    all_existing_res = None
    if skip_existing_scale_results:
        try:
            all_existing_res = pd.read_csv(results_filepath)
        except:
            pass

    for issue in issue_list:
        print('-- Scaling issue: ', issue)
        prompts = prompt_template.build_prompt(
            text, issue, use_examples=use_examples,
            override_persona_to_use=override_personas, override_encouragement_to_use=override_encouragements
        )

        for model in model_list:
            print('---- Scaling with model: ', model)
            prompts_to_use = []
            if skip_existing_scale_results and all_existing_res is not None:
                try:
                    for p in prompts:
                        p_persona = p.persona if res_persona == "text" else p.persona_idx
                        p_encouragement = p.encouragement if res_encouragement == "text" else p.encouragement_idx
                        existing_res = all_existing_res[
                            (all_existing_res['file'] == summary_filename)&
                            (all_existing_res['issue'] == issue) &
                            (all_existing_res['scale_model'] == model) &
                            (all_existing_res['persona'] == p_persona)&
                            (all_existing_res['encouragement'] == p_encouragement)
                            ]
                        if existing_res.shape[0] == 0:
                            prompts_to_use.append(p)
                        else:
                            print(f"Skip scale: {summary_filename}")
                except:
                    prompts_to_use = prompts
            else:
                prompts_to_use = prompts

            if len(prompts_to_use) > 0:

                results = scale_text_with_batch(
                    prompts_to_use, model, parse_retries=parse_retries, max_retries=max_retries,
                    concurrency=concurrency, probabilities=probabilities, dry_run=dry_run,
                    res_persona=res_persona, res_encouragement=res_encouragement
                )
                results_df = pd.DataFrame(results)
                results_df['issue'] = issue
                results_df['scale_model'] = model
                results_df['file'] = summary_filename
                results_df['created_at'] = datetime.now()
                for k, v in meta_columns.items():
                    results_df[k] = results_df[k] = v

                use_columns = [
                    'file', 'issue', 'scale_model', 'score', 'created_at',
                    *meta_columns.keys()
                ]

                if res_persona is not None:
                    use_columns.append('persona')
                if res_encouragement is not None:
                    use_columns.append('encouragement')
                if save_log:
                    use_columns+=['error_message', 'prompt', "responses"]

                scores_df = results_df[use_columns]

                # Writing to csv as we go to avoid losing data in case of an error
                if os.path.exists(results_filepath):
                    scores_df.to_csv(results_filepath, mode="a", index=False, header=False)
                else:
                    scores_df.to_csv(results_filepath, mode="w", index=False)

                overall_results.append(results_df)

    if len(overall_results) > 0:
        final_df = pd.concat(overall_results, axis=0)
        final_df = final_df.reset_index(drop=True)
        return final_df
