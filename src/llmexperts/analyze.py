import os
import numpy as np
import pandas as pd

from datetime import datetime
from langchain_core.load import dumps

from . import openai_model_list
from .model import LLMClient
from .prompts import AnalyzePromptTemplate, AnalyzedPrompt


def validate_score(score):
    return score in ['NA', '1', '2', '3', '4', '5', '6', '7']

def analyze_text_with_batch(
        prompt_list: list[AnalyzedPrompt], model, parse_retries=3, max_retries=7, concurrency=3,
        probabilities=False, dry_run=False
):
    """
    Analyzes the given text, given a list of prompts using the specified model.

    Args:
        prompt_list (list): A list of lists, each containing message objects representing the conversation.
        model (str): The name or ID of the model to use for analysis.
        parse_retries (int): The number of times to retry parsing the response. Defaults to 3.
        max_retries (int): The number of times to retry invoking the model. Defaults to 7, which should be enough
            to handle most TPM rate limits with langchains built in exponential backoff.
        concurrency (int): The number of concurrent requests to make to the model. Defaults to 3.
        probabilities (bool): Whether to include token probabilities in the response. Defaults to False. Only works with OpenAI models.
        dry_run (bool, optional): Don't invoke the LLM api call. Return a mock response for debug and testing. Defaults to False.

    Returns:
        dict: A dictionary containing the analyzed text generated by the model.

    """

    llm = LLMClient(model, max_tokens=150, temperature=0, max_retries=max_retries)

    # This is the core call to the model, using batch for concurrency
    # We needed to add concurrency because we hit rate limits with the API
    responses = []
    prompt_batches = [[p.prompt for p in prompt_list[i:i + concurrency]]
                      for i in range(0, len(prompt_list), concurrency)]

    for prompt_batch in prompt_batches:
        responses += llm.batch(prompt_batch, dry_run=dry_run, dry_run_res="NA")

    # This is hardcoded to expect a single score or NA in the response
    # If the desired response changes this will need to be updated
    # Originally this handled a json response but that was removed to make
    # this more robust.
    response_dicts = []
    responses_logs = []
    for i, response in enumerate(responses):
        response_parse = [response]
        try:
            score = response.content.strip()
            if not validate_score(score):
                raise ValueError(f'Invalid score: {score}')
            response_dict = {
                'score': score,
                'error_message': None,
                'prompt': dumps(prompt_list[i]),
                'persona': prompt_list[i].persona,
                'encouragement': prompt_list[i].encouragement,
            }
            if probabilities and (model in openai_model_list):
                try:
                    score = response_dict['score']
                    response_meta_df = pd.DataFrame(response.response_metadata["logprobs"]["content"])
                    score_metadata = response_meta_df[response_meta_df['token'] == str(score)].iloc[0]
                    # note this is a little fragile, it will retrieve the probability of the first token in the response
                    # that matches the score, which given the template "should" be the score itself, but it's not guaranteed
                    prob = np.exp(score_metadata['logprob'])
                    response_dict['prob'] = prob
                except Exception as e:
                    print(f'Error extracting probabilities from model {model}: {e}')
                    response_dict['prob'] = 'ERR'
        except Exception as original_error:
            attempt = 1
            while attempt <= parse_retries:
                print(
                    f'\nError parsing response from model {model}, retrying attempt {attempt}')
                try:
                    response = llm.invoke(prompt_list[i])
                    response_parse.append(response)
                    score = response.content.strip()
                    if not validate_score(score):
                        raise ValueError(f'Invalid score: {score}')
                    response_dict = {
                        'score': score, 'error_message': None,
                        'prompt': dumps(prompt_list[i]),
                        'persona': prompt_list[i].persona,
                        'encouragement': prompt_list[i].encouragement,
                    }
                    break
                except:
                    attempt += 1
            else:
                print(f'Retries failed with model {model}: {original_error}')
                response_dict = {
                    'score': 'ERR', 'error_message': original_error,
                    'prompt': dumps(prompt_list[i]),
                    'persona': prompt_list[i].persona,
                    'encouragement': prompt_list[i].encouragement,
                }
        responses_logs.append(response_parse)
        response_dict["responses"] = [dumps(r) for r in responses_logs]
        response_dicts.append(response_dict)
    return response_dicts


def analyze_file(
        filepath, model_list, issue_list, prompt_template: AnalyzePromptTemplate | os.PathLike, output_dir=None,
        parse_retries=3, max_retries=7, concurrency=3, probabilities=False,
        use_examples=False, override_personas=None, override_encouragements=None, dry_run=False,
        results_filepath=None, results_filename="analyze_results.csv",
        save_log=True, logs_filepath=None, logs_filename="analyze_logs.csv",
        meta_columns:dict=None, skip_existing_analyze_results=True
):
    """
    Analyzes a collection of text files using different models and prompts and output results in a single file.

    Args:
        filepath (str): Path to the file to be analyzed.
        model_list (list): A list of model names to use for analysis.
        issue_list (list): A list of issue areas corresponding to each text file.
        prompt_template (AnalyzePromptTemplate|os.PathLike): A SummarizePrompt instance or a filepath to the prompt file.
        output_dir (str): The path to the output directory where the results will be saved.
        parse_retries (int): The number of times to retry parsing the response. Defaults to 3.
        max_retries (int): The number of times to retry invoking the model. Defaults to 7, which should be enough
        to handle most TPM rate limits with langchains built in exponential backoff.
        concurrency (int): The number of concurrent requests to make to the model. Defaults to 3.
        probabilities (bool): Whether to include token probabilities in the response. Defaults to False. Only works with OpenAI models.
        use_examples (bool): Whether to add examples to the prompts. Defaults to False. Examples must be specified in the prompt_template.
        override_personas (int|list[int]): An index or a list of indices of personas to use. Persona texts will be fetched from the prompt template. If None, will use all personas in the template.
        override_encouragements (int|list[int]): An index or a list of indices of encouragements to use. encouragement texts will be fetched from the prompt template. If None, will use all encouragements in the template.
        dry_run (bool): Don't invoke the LLM api call. Return a mock response for debug and testing. Defaults to False.
        results_filepath (str): The path to the csv file where the results will be saved, if provided,
            output_dir and results_file_name will be ignored.
        results_filename (str): The name of the output file. Defaults to 'analyze_results.xlsx'.
        save_log (bool): Should the log information be saved to a file. Defaults to False.
        logs_filepath (str): The path to the log file where the scoring logs will be stored. If None and save_summary is True, save logs to the output dir.
        logs_filename (str): The name of the log file. Defaults to 'analyze_logs.csv'.
        meta_columns (dict): A dictionary of {column_name:value} which will be added to the final result file.
        skip_existing_analyze_results (bool): Whether to skip existing analyze_results that are already in the specified result file.

    Returns:
        DataFrame: A DataFrame containing the analyzed text generated by each model.

    """
    if not meta_columns:
        meta_columns = {}

    if results_filepath is None:
        if not os.path.exists(output_dir):
            os.makedirs(output_dir)
        results_filepath = os.path.join(output_dir, results_filename)
    else:
        if not os.path.exists(os.path.dirname(results_filepath)):
            os.makedirs(os.path.dirname(results_filepath))

    if logs_filepath is None:
        if not os.path.exists(output_dir):
            os.makedirs(output_dir)
        logs_filepath = os.path.join(output_dir, logs_filename)
    else:
        if not os.path.exists(os.path.dirname(logs_filepath)):
            os.makedirs(os.path.dirname(logs_filepath))

    summary_filename = os.path.basename(filepath)

    overall_results = []
    # Loop through each file, issue area, model and prompt
    if not isinstance(prompt_template, AnalyzePromptTemplate):
        prompt_template = AnalyzePromptTemplate.from_file(prompt_template)

    # for file_name in file_list:
    print('--------\nAnalyzing file: ', summary_filename)
    with open(filepath, "r", encoding="utf-8") as file:
        text = file.read()

    all_existing_res = None
    if skip_existing_analyze_results:
        try:
            all_existing_res = pd.read_csv(results_filepath)
        except:
            pass

    for issue in issue_list:
        print('-- Analyzing issue: ', issue)
        prompts = prompt_template.build_prompt(
            text, issue, use_examples=use_examples,
            override_persona_to_use=override_personas, override_encouragement_to_use=override_encouragements
        )

        for model in model_list:
            print('---- Analyzing with model: ', model)
            prompts_to_use = []
            if skip_existing_analyze_results and all_existing_res is not None:
                try:
                    for p in prompts:
                        p_persona = p.persona
                        p_encouragement = p.encouragement
                        existing_res = all_existing_res[
                            (all_existing_res['file'] == summary_filename)&
                            (all_existing_res['issue'] == issue) &
                            (all_existing_res['analyze_model'] == model) &
                            (all_existing_res['persona'] == p_persona)&
                            (all_existing_res['encouragement'] == p_encouragement)
                            ]
                        if existing_res.shape[0] == 0:
                            prompts_to_use.append(p)
                        else:
                            print(f"Skip analyze: {summary_filename}")
                except:
                    prompts_to_use = prompts
            else:
                prompts_to_use = prompts

            if len(prompts_to_use) > 0:

                results = analyze_text_with_batch(
                    prompts_to_use, model, parse_retries=parse_retries, max_retries=max_retries,
                    concurrency=concurrency, probabilities=probabilities, dry_run=dry_run
                )
                results_df = pd.DataFrame(results)
                results_df['issue'] = issue
                results_df['analyze_model'] = model
                results_df['file'] = summary_filename
                results_df['created_at'] = datetime.now()
                for k, v in meta_columns.items():
                    results_df[k] = results_df[k] = v
                scores_df = results_df[[
                    'file', 'issue', 'analyze_model', 'persona', 'encouragement', 'score', 'created_at', *meta_columns.keys()]]

                logs_df = results_df[[
                    'file', 'issue', 'analyze_model', 'persona', 'encouragement', 'score', 'error_message', 'prompt', "responses", 'created_at', *meta_columns.keys()]]

                # Writing to csv as we go to avoid losing data in case of an error
                if os.path.exists(results_filepath):
                    scores_df.to_csv(results_filepath, mode="a", index=False, header=False)
                else:
                    scores_df.to_csv(results_filepath, mode="w", index=False)

                if save_log:
                    if os.path.exists(logs_filepath):
                        logs_df.to_csv(logs_filepath, mode="a", index=False, header=False)
                    else:
                        logs_df.to_csv(logs_filepath, mode="w", index=False)

                overall_results.append(results_df)

    if len(overall_results) > 0:
        final_df = pd.concat(overall_results, axis=0)
        final_df = final_df.reset_index(drop=True)
        return final_df
